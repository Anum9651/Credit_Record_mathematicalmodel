# -*- coding: utf-8 -*-
"""A_mathematical_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zInkmyW1MV7baVts-yHeroBXlltKXdzE
"""

import os
import pandas as pd

# Check the current working directory
print("Current Working Directory:", os.getcwd())

# If using Google Colab, you can upload files
try:
    from google.colab import files
    uploaded = files.upload()
except ImportError:
    print("Not running in Google Colab. Please ensure your CSV files are in the correct directory.")

# Attempt to load the datasets
try:
    application_data = pd.read_csv('application_record.csv')
    credit_data = pd.read_csv('credit_record.csv')

    print(application_data.head())
    print(credit_data.head())
except FileNotFoundError as e:
    print(f"Error: {e}")

data = np.random.rand(100, 2)  # Example with fewer data points.

#EXPT-5
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Create a smaller test dataset
np.random.seed(42)
data = np.random.rand(20, 2)  # Reduced dataset size for testing

# Perform hierarchical clustering
linkage_matrix = linkage(data, method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 5))
dendrogram(linkage_matrix, truncate_mode='lastp', p=10)  # Truncate to show last 10 merges
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Generate synthetic data
n_samples = 500
n_clusters = 3
X, _ = make_blobs(n_samples=n_samples, centers=n_clusters, cluster_std=0.60, random_state=42)

# Plot the generated data
plt.scatter(X[:, 0], X[:, 1], s=30)
plt.title('Generated Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# K-means Clustering
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=30, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, label='Centroids')
plt.title('K-means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# Gaussian Mixture Model
gmm = GaussianMixture(n_components=n_clusters)
gmm.fit(X)
y_gmm = gmm.predict(X)

plt.scatter(X[:, 0], X[:, 1], c=y_gmm, s=30, cmap='viridis')
plt.title('Gaussian Mixture Model Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=n_clusters)
y_hierarchical = hierarchical.fit_predict(X)

plt.scatter(X[:, 0], X[:, 1], c=y_hierarchical, s=30, cmap='viridis')
plt.title('Hierarchical Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Dendrogram
linked = linkage(X, 'ward')
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

#EXPT - 6
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
import seaborn as sns

# Load data (replace 'application_record.csv' with your dataset)
application_data = pd.read_csv('application_record.csv',
                                 dtype={'CODE_GENDER': 'category',
                                        'FLAG_OWN_CAR': 'category',
                                        'FLAG_OWN_REALTY': 'category'})

# Sample data if necessary
application_data = application_data.sample(frac=0.1, random_state=42)

# Drop non-numeric and categorical data for PCA
numeric_data = application_data.select_dtypes(include=[np.number])

# Handle missing values
numeric_data.fillna(numeric_data.mean(), inplace=True)

# Normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(numeric_data)

# Perform PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions
X_pca = pca.fit_transform(X_scaled)

# Create a DataFrame for PCA results with explained variance ratio
pca_df = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])
explained_variance = pca.explained_variance_ratio_

# Plot PCA results
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Principal Component 1', y='Principal Component 2', data=pca_df)
plt.title('PCA of Dataset')
plt.xlabel('Principal Component 1 ({}%)'.format(round(explained_variance[0] * 100, 2)))
plt.ylabel('Principal Component 2 ({}%)'.format(round(explained_variance[1] * 100, 2)))
plt.grid()
plt.show()

# Display explained variance
print('Explained variance ratio:', explained_variance)
print('Total explained variance by the two components:', sum(explained_variance) * 100)

pip install hmmlearn

#EXPT-7
import numpy as np
import matplotlib.pyplot as plt
from hmmlearn import hmm

# Generate synthetic sequential data
np.random.seed(42)
n_samples = 1000

# Create an array of random observations
# We will simulate this for a single feature for simplicity
observations = np.concatenate([
    np.random.normal(loc=0., scale=1., size=n_samples // 2),
    np.random.normal(loc=5., scale=1., size=n_samples // 2)
]).reshape(-1, 1)

# Create an HMM model
# Here we define 2 hidden states
model = hmm.GaussianHMM(n_components=2, covariance_type="full")

# Fit the HMM model
model.fit(observations)

# Predict the hidden states
hidden_states = model.predict(observations)

# Visualize the Observations and the Hidden States
plt.figure(figsize=(15, 10))
plt.subplot(2, 1, 1)
plt.title('Observations')
plt.plot(observations, color='blue', label='Observations')
plt.ylabel('Value')
plt.legend()

plt.subplot(2, 1, 2)
plt.title('Hidden States')
plt.plot(hidden_states, color='orange', label='Hidden States')
plt.ylabel('Hidden State')
plt.xlabel('Time Step')
plt.legend()

plt.tight_layout()
plt.show()

# Making predictions
n_future_samples = 100
hidden_states_future = model.predict(np.random.normal(loc=2.5, scale=1., size=n_future_samples).reshape(-1, 1))

# Visualize future state predictions
plt.figure(figsize=(8, 4))
plt.plot(hidden_states_future, color='green', label='Predicted Hidden States for Future')
plt.title('Future Predicted Hidden States')
plt.ylabel('Hidden State')
plt.xlabel('Future Time Step')
plt.legend()
plt.show()

# Display model parameters
print("Transition matrix:\n", model.transmat_)
print("Means of each hidden state:\n", model.means_)
print("Covariance of each hidden state:\n", model.covars_)

import os
import pandas as pd

# Check the current working directory
print("Current Working Directory:", os.getcwd())

# If using Google Colab, you can upload files
try:
    from google.colab import files
    uploaded = files.upload()
except ImportError:
    print("Not running in Google Colab. Please ensure your CSV files are in the correct directory.")

# Attempt to load the datasets
try:
    application_data = pd.read_csv('application_record.csv')
    credit_data = pd.read_csv('credit_record.csv')

    print(application_data.head())
    print(credit_data.head())
except FileNotFoundError as e:
    print(f"Error: {e}")

#EXPT 8 - Implement CART learning algorithms to perform categorization.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Load the data
application_df = pd.read_csv('application_record.csv')  # Update with your filename
credit_df = pd.read_csv('credit_record.csv')  # Update with your filename

# Step 2: Explore the data
print(application_df.head())  # Display the first few rows of the application DataFrame
print(credit_df.head())  # Display the first few rows of the credit record DataFrame

# Step 3: Data Preprocessing
# Create a target variable based on the credit record (e.g., classify if there are any defaults)
# Here we assume `STATUS` is the target variable. We have to consolidate it to one record per client.
# Convert STATUS to binary (1 if defaulted, 0 otherwise)
credit_df['TARGET'] = credit_df['STATUS'].apply(lambda x: 1 if x in ['1', '2', '3'] else 0)  # Modify as per your logic

# Group by ID, assuming that the latest month has the most relevant information
target_df = credit_df.groupby('ID')['TARGET'].max().reset_index()

# Merge with application DataFrame to include features
data = pd.merge(application_df, target_df, on='ID', how='left')

# Fill NaN values in the TARGET column (assuming NaN indicates no default)
data['TARGET'].fillna(0, inplace=True)

# Step 4: Define features and target
X = data.drop(columns=['ID', 'TARGET'])  # Drop ID and target variable from features
y = data['TARGET']  # Target variable

# One-hot encode categorical features
X = pd.get_dummies(X, drop_first=True)

# Step 5: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train the CART model
cart_model = DecisionTreeClassifier(random_state=42)
cart_model.fit(X_train, y_train)

# Step 7: Make predictions
y_pred = cart_model.predict(X_test)

# Step 8: Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

#EXPT 9 - Implement ensemble learning models to perform classification.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Load the data
application_df = pd.read_csv('application_record.csv')  # Update with your filename
credit_df = pd.read_csv('credit_record.csv')  # Update with your filename

# Step 2: Data Preprocessing (same as previous)
credit_df['TARGET'] = credit_df['STATUS'].apply(lambda x: 1 if x in ['1', '2', '3'] else 0)  # Modify as per your logic
target_df = credit_df.groupby('ID')['TARGET'].max().reset_index()
data = pd.merge(application_df, target_df, on='ID', how='left')
data['TARGET'] = data['TARGET'].fillna(0)  # Fill NaNs, assuming NaN indicates no default

# Define features and target
X = data.drop(columns=['ID', 'TARGET'])
y = data['TARGET']

# One-hot encode categorical features
X = pd.get_dummies(X, drop_first=True)

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train Ensemble Learning Models

# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# AdaBoost (Corrected)
ada_model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), random_state=42)
ada_model.fit(X_train, y_train)
y_pred_ada = ada_model.predict(X_test)

# Step 5: Evaluate the models
print("Random Forest - Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))
print("\nRandom Forest - Classification Report:")
print(classification_report(y_test, y_pred_rf))

print("AdaBoost - Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_ada))
print("\nAdaBoost - Classification Report:")
print(classification_report(y_test, y_pred_ada))